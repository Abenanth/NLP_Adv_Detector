# NLP_Adversial_Prompt_Detector

![image](https://github.com/user-attachments/assets/6bf192e6-4246-48b5-9fe8-ee166c8fa4bc)

# Here I used GPT-4 any LLM can be used in a similar way to safegaurd it from JailBreak Attemps. Plus we can always fine-tune the classifiers accordingly.
# Here I used AWS Sagemaker for fine-tuning both DistilBERT and RoBERTa models.

